
UMT2013 
LLNL-CODE-638452
Title: UMT, Version:  2.0


NOTICE 1
This work was produced at the Lawrence Livermore National Laboratory (LLNL) 
under contract no. DE-AC-52-07NA27344 (Contract 44) between the U.S. Department of Energy (DOE) 
and Lawrence Livermore National Security, LLC (LLNS) for the operation of LLNL. 
The rights of the Federal Government are reserved under Contract 44.
                                            
DISCLAIMER
This work was prepared as an account of work sponsored by an agency of the United States Government. 
Neither the United States Government nor Lawrence Livermore National Security, LLC 
nor any of their employees, makes any warranty, express or implied, or assumes any liability 
or responsibility for the accuracy, completeness, or usefulness of any information, 
apparatus, product, or process disclosed, or represents that its use 
would not infringe privately-owned rights. 
Reference herein to any specific commercial products, process, or service by 
trade name, trademark, manufacturer or otherwise does not necessarily constitute or 
imply its endorsement, recommendation, or favoring by the United States Government or 
Lawrence Livermore National Security, LLC. 
The views and opinions of authors expressed herein do not necessarily state or reflect 
those of the United States Government or Lawrence Livermore National Security, LLC, 
and shall not be used for advertising or product endorsement purposes.
                                    
NOTIFICATION OF COMMERCIAL USE
Commercialization of this product is prohibited without notifying the Department of Energy (DOE) 
or Lawrence Livermore National Security.

DESCRIPTION OF UMT2013

UMT2013 is an LLNL ASC proxy application (mini-app) that performs three-dimensional, non-linear, 
radiation transport calculations using deterministic methods.
The method of solution is as follws. UMT performs the solution of time-dependent, energy-dependent, 
discrete ordinates, and nonlinear radiation problems on three-dimensional unstructured spatial grids 
on large distributed-memory multi-node parallel computer systems with multiple cores per node. 
To achieve extreme scalability, the application exploits both spatial decomposition 
using message passing between nodes and a threading algorithm in angle within the node.

The project addresses the need to obtain accurate transport solutions to three-dimensional 
radiation transfer problems (i.e. the transfer of thermal photons). 
The solution calls for the use of an unstructured grid. 
This class of problems is characterized by tens of thousands of unknowns per zone 
and upwards of millions of zones, thus requiring large, scalable, parallel computing platforms. 
The package uses a combination of message passing and threading, 
utilizes the large distributed memory of the platform and provides unprecedented (weak) scaling 
to very large core counts.

The application will execute on all modern High Performance Computing  (HPC) systems. 
At LLNL we have been using IBM BG/Q systems and Intel Xeon (Sandybridge) clusters to 
develop and test the software,

The number of MPI ranks that can be launched on a compute node is limited by the size 
of the memory available on the node. 

The package is designed to take advantage of MPI message passing as well as threading using OpenMP. 
Consequently systems that have efficient MPI and OpenMP software are better suited for this application, 
especially at scale.

It is best if the compute nodes have enough DRAM to provide about 1GB or 2GB per core.
For systems with 1GB of memory per core, fewer MPI ranks will fit in the node than there are cores, 
for moderate size problems. Memory footprint will be discussed elsewhere in this README file.

The time required to solve problems is highly dependent on the details of the problem, 
but the software is designed to reduce time to solution with the use of a 
spatially decomposed and threaded algorithm.

The performance of platforms is tested with a weak scaling experiment. 
A figure of merit will be computed.
The performance of platforms is tested with a weak scaling experiment. 
A figure of merit (FOM) will be computed. This is done in main() and can be perused in the file SuOlsonTest.cc
FOM is number_Unknowns/cumulativeWorkTime*cumulativeIterationCount.
The FOM will grow as more and more MPI ranks are added and as more and more OpenMP threads are added.
There is more on executing UMT2013 below.


REFERENCE

P.F. Nowak and M.K. Nemanic, "Radiation Transport Calculations on Unstructured
Grids Using a Spatially Decomposed and Threaded Algorithm," Proc. Int. Conf.
Mathematics and Computation, Reactor Physics and Environmental Analysis in
Nuclear Applications, Madrid, Spain, September 27-30, 1999, Vol. 1, p. 379 (1999).

Paul Nowak, "Deterministic Methods for Radiation  Transport: Lessons Learned and Future Directions", 
ASC Workshop on Methods for Computational Physics and Modern Software Practices, March 2004, 
LLNL document UCRL-CONF-202912.

WHAT IS INCLUDED IN TARBALL

1. Source code (F90 and C++)
2. Input files (e.g. grid_32768_12x12x12.cmg, grid_16384_12x12x12.cmg, grid_8192_12x12x12.cmg etc.)
3. Run scripts for launching SuOlsonTest
   run_1rack_8Kmpi_8omp_8rpn  
   run_2rack_16Kmpi_8omp_8rpn  
   run_4rack_32Kmpi_8omp_8rpn
   plus others.
4. README
5. Typical output

BUILDING UMT2013

If your home directory is /home/einstein/,
afetr you untar the file you will have a directory /home/einstein/UMT2013.
The steps to build and executable called SuOlsonTest are:

1. cd /home/einstein/UMT2013
2. Modify make.defs to reflect the platform's compilers, compiler options, libraries, MPI wrappers etc. 
   A working/tested example is provided.
3. gmake clean
4. gmake
5. The following libraries or their ".a" counterparts should exist at this point: 
   ./CMG_CLEAN/src/libcmgp.so
   ./Teton/libTetonUtils.so
   ./Teton/libInfrastructure.so
   ./cmg2Kull/sources/libc2k.so
   Note:
   The code is designed to use MPI and OpenMP.
   SIMD and/or other vectorization may be turned on and is encouraged.

   Please note that for thread_safety, THREE  changes have been made (on 3 Feb 2014) 
   to the source that was distributed during 2013, as follows:
   Change #1 
   In Teton/transport/Teton/mods/ZoneData_mod.F90
   replace 
     type(ZoneData), pointer, public :: Z
   with
     type(ZoneData), pointer, public :: Z
     !$OMP threadprivate(Z)

   Change #2 
   In Teton/transport/Teton/mods/Quadrature_mod.F90
   replace
     type(Exit),       pointer, public :: ExitBdy
   with
     type(Exit),       pointer, public :: ExitBdy
     !$OMP threadprivate (ExitBdy)

   Change #3 
   In Teton/transport/Teton/mods/Boundary_mod.F90
   replace
     type(Bdy),       pointer, public :: ExitBdy
   with
     type(Bdy),       pointer, public :: ExitBdy
     !$OMP threadprivate (Bdy)


   Your downloaded tarfile should have these changes.

6. cd ./Teton (now in /home/einstein/UMT2013/Teton)
   gmake SuOlsonTest 
7. The following executable should exist at this point: 
   /home/einstein/UMT2013/Teton/SuOlsonTest

EXECUTING UMT2013

Bidders will be expected to execute the executable ./SuOlsonTest.
In the Teton directory, we have included several run scripts to launch SuOlsonTest:
    run_1rack_8Kmpi_8omp_8rpn 
    run_2rack_16Kmpi_8omp_8rpn 
    run_4rack_32Kmpi_8omp_8rpn

The example used in this paragraph pertains to 1 rack of LLNL's IBM BG/Q system.
A rack has 1K nodes. A node has 16 cores, 4 HW threads and 16 GB.
The problem size for this run was 12x12x12 zones per MPI rank.
The node footprint (to be discussed below) of the MPI job allowed use of 8 ranks per node (rpn).
The single rack of BG/Q was used to execute with 8K MPI ranks and 8 OMP threads because there are 
a total of 64 (#MPI*#OMP) resources available on the node.

Since the BG/Q is using SLURM job scheduling (srun), a typical launch command could be:

Example 1 - 1 rack, 1K nodes/rack, 8 MPI per node
export OMP_NUM_THREADS=8
srun -n 8192 -N 1024 ./SuOlsonTest $gridFileName $Groups $quadType $Order $Polar $Azim
where:
    gridFileName=grid_8192_12x12x12.cmg   # this file is included.
    Order=16
    Groups=16
    quadType=2
    Polar=8
    Azim=4

The structure of the file grid_8192_12x12x12.cmg will allow bidders to easily construct other 
input files, larger or smaller in zone size or processor grid size.
The processor grid is defined by sms(16,32,16) in the file grid_8192_12x12x12.cmg
The number of zones per rank is set by using numzones(12,12,12) in the file grid_8192_12x12x12.cmg
There are other specifications too.

For 2-racks the launch command would be
export OMP_NUM_THREADS=8
srun -n 16384 -N 2048 ./SuOlsonTest $gridFileName $Groups $quadType $Order $Polar $Azim
where
    gridFileName=grid_16384_12x12x12.cmg   # this file is included.
The processor grid is defined by sms(32,32,16) in the file grid_16384_12x12x12.cmg

For 4-racks the launch command would be
export OMP_NUM_THREADS=8
srun -n 32768 -N 4096 ./SuOlsonTest $gridFileName $Groups $quadType $Order $Polar $Azim
where:
    gridFileName=grid_32768_12x12x12.cmg   # this file is included.
The processor grid is defined by sms(32,32,32) in the file grid_16384_12x12x12.cmg
This case uses a total of 256K threads (i.e. 32768 MPI *8 OMP).

Example 2 - 1 rank per core, 16 cores per node, 32 nodes, 1 OMP thread per rank 
(as used in LLNL's Intel Sandybrige cluster)
export OMP_NUM_THREADS=1
srun -N 32 -n 512 ./SuOlsonTest $gridFileName $Groups $quadType $Order $Polar $Azim
and the relevant grid file would contain 
sms(8,8,8)
numzones(12,12,12)
plus other appropriate specifications.

Example 3 - Large problems that uses 8 and 16 BG/Q racks
8 rack: srun -n 65536 -N 8192 ./SuOlsonTest $gridFileName $Groups $quadType $Order $Polar $Azim 
with sms(64,32,32)
16 rack: srun -n 131072 -N 16384 ./SuOlsonTest $gridFileName $Groups $quadType $Order $Polar $Azim
with sms(64,64,32)


Memory Use

Memory needed per rank will be around 
2*nZones *8*nGroup*nOrder*(nOrder+2)*8 bytes + 6*Nx*Ny*4*nGroup*nOrder*(nOrder+2)* 8 bytes
On our TLCC2 cluster with Intel Sandybridge cores, 16 cores/node, 2GB/core, 32 GB/node, 
we executed 16 MPI tasks on a node - each rank used 1.2GB.
It was possible to launch 16 MPI tasks on our 32GB nodes.
On our other supercomputer with 16 GB/node (16 cores) we can only fit 8 MPI ranks solving a 12x12x12 problem 
with 16 groups and order 16. So we would use 8 MPI tasks and 8 threads since the node allows 64 way parallelism.


Weak Scaling

Weak scaling would scale indefinitely and the FOM number would reflect that. 
I have only tried up to 4 racks with 32K MPI and 8 OMP threads. 
Million way parallelism is probably within your reach if you can efficiently deploy OMP threads.


Parallelism

OpenMP parallelism is deployed in the F90 function snflwxyz in file Teton/transport/Teton/snac/snflwxyz.F90.
The available parallelism equals the product Polar*Azim (i.e. 32 in our Example 1 above)


UMT2013 Performance (BG/Q) - Figure of Merit

BG/Q PowerPC A2 Clock Frequency 1.6 GHz.							
problem size 12x12x12 (#zones/MPI rank is slightly bigger)

Racks #Nodes    #MPI    Processor      #MPI    #OMP     Total   BGQ FOM
per             ranks   grid           per             MPI*OMP
Node                                   Node
1	1024	8192	16x32x16	8	1	8192	1.93E+10
1	1024	8192	16x32x16	8	2	16384	3.52E+10
1	1024	8192	16x32x16	8	4	32768	5.13E+10
1	1024	8192	16x32x16	8	8	65536	7.06E+10
							
2	2048	16384	32x32x16	8	1	16384	3.86E+10
2	2048	16384	32x32x16	8	2	32768	7.16E+10
2	2048	16384	32x32x16	8	4	65536	1.11E+11
2	2048	16384	32x32x16	8	8	131072	1.42E+11
							
4	4096	32768	32x32x32	8	1	32768	6.86E+10
4	4096	32768	32x32x32	8	2	65536	1.29E+11
4	4096	32768	32x32x32	8	4	131072	2.09E+11
4	4096	32768	32x32x32	8	8	262144	2.58E+11

Memory Footprint per MPI task = 1.2 GB per rank.							
Node memory = 16 GB.							
Compilers: IBM mpixlcxx_r version 12.1, mpi_xlc_r version 12.1  and IBM mpixlf90_r version 14.1
MPI: mvapich2 (based on 1.4)							
OpenMP: IBM XL compilers support OpenMP 3.1 (LOMP was NOT used)							
CFLAGS_OPT = -g -O2 -qsmp=omp -qmaxmem=256000 -g -w -DNDEBUG -qpic=large -qlanglvl=extended
CXXFLAGS_OPT = -g -O2 -qsmp=omp -qmaxmem=256000 -g  -w -DNDEBUG -qpic=large 
F90FLAGS_OPT  = -g -O2 -qsmp=omp -qsimd=noauto -qsuffix=cpp=F90 -qfree=f90 -qsuffix=f=f90 -qmaxmem=256000 -qpic=large
LDFLAGS = -q -qsmp=omp -qmkshrobj -qnostaticlink -qpic=large							



VERIFICATION & VALIDATION

A typical output file is provided: Intel_SNB_64MPIx8omp_32nodes.out
This file was the output of an execution of SuOlsonTest with 64 MPI tasks on 32 nodes (16cores/node) 
and 8 OMP threads per rank.
The executable was built with these options:
F90FLAGS_OPT    =  -g -O3 -no-prec-div  -fPIC -openmp
CXXFLAGS_OPT    =  -g -O3 -w  -no-prec-div -fPIC  -openmp
CFLAGS_OPT      =  -g -O3 -w -no-prec-div -fPIC  -openmp  


Three other output files from our BGQ 1-rack, 2-rack and 3-rack execution are also included:
1rack_8192_16x32x16_8_rpn_OMPperf2.out
2rack_16384_32x32x16_8_rpn_OMPperf2.out
4rack_32768_32x32x32_8_rpn_OMPperf2.out

